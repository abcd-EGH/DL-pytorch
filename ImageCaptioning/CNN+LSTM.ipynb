{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f349b8-ccc5-4625-82c2-ecc8cb1538d6",
   "metadata": {},
   "source": [
    "# Download Dataset & Extract Zip File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c527e-c863-478c-82d6-fef3167bc675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# COCO Dataset 다운로드\n",
    "\n",
    "import urllib\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "#https://stackoverflow.com/a/53877507/1558946\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "def download_data(url):\n",
    "    print(f\"{url} 다운로드 중 ...\")\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        zip_path, _ = urllib.request.urlretrieve(url, reporthook=t.update_to)\n",
    "\n",
    "    print(\"압축을 푸는 중 ...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "        for name in tqdm(iterable=f.namelist(), total=len(f.namelist())):\n",
    "            f.extract(member=name, path=\"data_dir\")\n",
    "\n",
    "\n",
    "download_data(\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\")\n",
    "download_data(\"http://images.cocodataset.org/zips/train2014.zip\")\n",
    "download_data(\"http://images.cocodataset.org/zips/val2014.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908e517-9cfa-477e-b212-ad8ed2044a29",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a66548-4e4f-47d0-ba33-6bfa08b068e2",
   "metadata": {},
   "source": [
    "## Text (Caption) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a4cf18-53bd-4fb0-8f09-4b1ee31efeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a8622-bb23-4086-9528-65eb44e079f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# punkt tokenizer 모델을 통해 주어진 텍스트를 구성된 단어로 토큰화\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d90ac7be-ff61-404e-99ce-f737e38740f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.w2i = {}\n",
    "        self.i2w = {}\n",
    "        self.index = 0\n",
    " \n",
    "    def __call__(self, token):\n",
    "        if not token in self.w2i:\n",
    "            return self.w2i['<unk>']\n",
    "        return self.w2i[token]\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.w2i)\n",
    "    def add_token(self, token):\n",
    "        if not token in self.w2i:\n",
    "            self.w2i[token] = self.index\n",
    "            self.i2w[self.index] = token\n",
    "            self.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77c11533-ee99-4dde-837a-613537cbbe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.74s)\n",
      "creating index...\n",
      "index created!\n",
      "[100000/414113] Tokenized the captions.\n",
      "[200000/414113] Tokenized the captions.\n",
      "[300000/414113] Tokenized the captions.\n",
      "[400000/414113] Tokenized the captions.\n",
      "Total vocabulary size: 9948\n",
      "Saved the vocabulary wrapper to 'D:\\pytorch-study\\ImageCaptioning\\data_dir\\vocabulary.pkl'\n"
     ]
    }
   ],
   "source": [
    "# 실제 텍스트 토큰(단어 등)을 숫자 토큰으로 전환할 수 있는 사전 구축\n",
    "def build_vocabulary(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    # Load JSON Text Annotation\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        # Tokenize each word in Annotation/Caption (Convert to numbers) and Save to Counter\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    " \n",
    "        if (i+1) % 100000 == 0:\n",
    "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
    " \n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    tokens = [token for token, cnt in counter.items() if cnt >= threshold]\n",
    " \n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocab()\n",
    "    vocab.add_token('<pad>')\n",
    "    vocab.add_token('<start>')\n",
    "    vocab.add_token('<end>')\n",
    "    vocab.add_token('<unk>')\n",
    " \n",
    "    # Add the words to the vocabulary.\n",
    "    for i, token in enumerate(tokens):\n",
    "        vocab.add_token(token)\n",
    "    return vocab\n",
    " \n",
    "vocab = build_vocabulary(json='data_dir/annotations/captions_train2014.json', threshold=4)\n",
    "vocab_path = './data_dir/vocabulary.pkl'\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f51b3f-84d2-49ad-8f4d-ab831c5d26cf",
   "metadata": {},
   "source": [
    "## Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe2715e8-ae63-4ee9-a644-be83ea16c769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10000/82783] Resized the images and saved into 'D:\\pytorch-study\\ImageCaptioning\\data_dir\\resized_images/'.\n",
      "[20000/82783] Resized the images and saved into 'D:\\pytorch-study\\ImageCaptioning\\data_dir\\resized_images/'.\n",
      "[30000/82783] Resized the images and saved into 'D:\\pytorch-study\\ImageCaptioning\\data_dir\\resized_images/'.\n",
      "[40000/82783] Resized the images and saved into 'D:\\pytorch-study\\ImageCaptioning\\data_dir\\resized_images/'.\n",
      "[50000/82783] Resized the images and saved into 'D:\\pytorch-study\\ImageCaptioning\\data_dir\\resized_images/'.\n",
      "[60000/82783] Resized the images and saved into 'D:\\pytorch-study\\ImageCaptioning\\data_dir\\resized_images/'.\n",
      "[70000/82783] Resized the images and saved into 'D:\\pytorch-study\\ImageCaptioning\\data_dir\\resized_images/'.\n",
      "[80000/82783] Resized the images and saved into 'D:\\pytorch-study\\ImageCaptioning\\data_dir\\resized_images/'.\n"
     ]
    }
   ],
   "source": [
    "# 고정된 image_shape로 reshape\n",
    "def reshape_image(image, shape):\n",
    "    \"\"\"Resize an image to the given shape.\"\"\"\n",
    "    # return image.resize(shape, Image.ANTIALIAS) # Pillow 10.0.0에서 'ANTIALIAS'의 모듈이 제거되었음\n",
    "    return image.resize(shape, Image.LANCZOS)\n",
    " \n",
    "def reshape_images(image_path, output_path, shape):\n",
    "    \"\"\"Reshape the images in 'image_path' and save into 'output_path'.\"\"\"\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    " \n",
    "    images = os.listdir(image_path)\n",
    "    num_im = len(images)\n",
    "    for i, im in enumerate(images):\n",
    "        with open(os.path.join(image_path, im), 'r+b') as f:\n",
    "            with Image.open(f) as image:\n",
    "                image = reshape_image(image, shape)\n",
    "                image.save(os.path.join(output_path, im), image.format)\n",
    "        if (i+1) % 10000 == 0:\n",
    "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
    "                   .format(i+1, num_im, output_path))\n",
    "\n",
    "image_path = './data_dir/train2014/'\n",
    "output_path = './data_dir/resized_images/'\n",
    "image_shape = [256, 256]\n",
    "reshape_images(image_path, output_path, image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8951e90d-b603-4fc0-8c69-329f000cde2e",
   "metadata": {},
   "source": [
    "# Define Image Caption Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2f55916-f837-40ca-8b1a-ba7e6f60dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCocoDataset(data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, data_path, coco_json_path, vocabulary, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = data_path\n",
    "        self.coco_data = COCO(coco_json_path)\n",
    "        self.indices = list(self.coco_data.anns.keys())\n",
    "        self.vocabulary = vocabulary\n",
    "        self.transform = transform\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        coco_data = self.coco_data\n",
    "        vocabulary = self.vocabulary\n",
    "        annotation_id = self.indices[idx]\n",
    "        caption = coco_data.anns[annotation_id]['caption']\n",
    "        image_id = coco_data.anns[annotation_id]['image_id']\n",
    "        image_path = coco_data.loadImgs(image_id)[0]['file_name']\n",
    " \n",
    "        image = Image.open(os.path.join(self.root, image_path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    " \n",
    "        # Convert caption (string) to word ids.\n",
    "        word_tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocabulary('<start>'))\n",
    "        caption.extend([vocabulary(token) for token in word_tokens])\n",
    "        caption.append(vocabulary('<end>'))\n",
    "        ground_truth = torch.Tensor(caption)\n",
    "        return image, ground_truth\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "977355a7-524f-40d3-b618-399eacb797ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(data_batch):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data_batch.sort(key=lambda d: len(d[1]), reverse=True)\n",
    "    imgs, caps = zip(*data_batch)\n",
    " \n",
    "    # Merge images (from list of 3D tensors to 4D tensor).\n",
    "    # Originally, imgs is a list of <batch_size> number of RGB images with dimensions (3, 256, 256)\n",
    "    # This line of code turns it into a single tensor of dimensions (<batch_size>, 3, 256, 256)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    " \n",
    "    # Merge captions (from list of 1D tensors to 2D tensor), similar to merging of images donw above.\n",
    "    cap_lens = [len(cap) for cap in caps]\n",
    "    tgts = torch.zeros(len(caps), max(cap_lens)).long()\n",
    "    for i, cap in enumerate(caps):\n",
    "        end = cap_lens[i]\n",
    "        tgts[i, :end] = cap[:end]        \n",
    "    return imgs, tgts, cap_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c0bebc3-42ad-40b7-8fbe-9a490be29ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO Dataset을 위한 맞춤형 데이터 로더를 반환하는 get_loader 함수 구현\n",
    "def get_loader(data_path, coco_json_path, vocabulary, transform, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco_dataset = CustomCocoDataset(data_path=data_path,\n",
    "                       coco_json_path=coco_json_path,\n",
    "                       vocabulary=vocabulary,\n",
    "                       transform=transform)\n",
    "    \n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    custom_data_loader = torch.utils.data.DataLoader(dataset=coco_dataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_function)\n",
    "    return custom_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5931e7b-d0f2-45b6-aed1-8240a1a1d5ed",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0d483-1b9c-40b0-94ac-da6cf2a7d5fe",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80cc2415-1e41-4ae5-b933-e96ba17d0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(CNNModel, self).__init__()\n",
    "        efficientnet = models.efficientnet_b4(weights='IMAGENET1K_V1')\n",
    "        module_list = list(efficientnet.children())[:-1]      # delete the last dropout & fc layer.\n",
    "        last_layer = list(efficientnet.children())[-1]      # fc layer of efficientnet = list(efficientnet.children())[-1]\n",
    "        self.efficientnet_module = nn.Sequential(*module_list, last_layer[0]) # add the pretrained last dropout layer.\n",
    "        self.linear_layer = nn.Linear(last_layer[1].in_features, 256) # \n",
    "        self.batch_norm = nn.BatchNorm1d(256, momentum=0.01)\n",
    "        \n",
    "    def forward(self, input_images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            resnet_features = self.efficientnet_module(input_images)\n",
    "        resnet_features = resnet_features.reshape(resnet_features.size(0), -1)\n",
    "        final_features = self.batch_norm(self.linear_layer(resnet_features))\n",
    "        return final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e717bf-b7d1-4705-828b-24e1aca53573",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2393be34-ec81-43f0-bd04-c8339d6fef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_layer_size, vocabulary_size, num_layers, max_seq_len=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.lstm_layer = nn.LSTM(embedding_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.linear_layer = nn.Linear(hidden_layer_size, vocabulary_size)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def forward(self, input_features, capts, lens):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embedding_layer(capts)\n",
    "        embeddings = torch.cat((input_features.unsqueeze(1), embeddings), 1)\n",
    "        lstm_input = pack_padded_sequence(embeddings, lens, batch_first=True) \n",
    "        hidden_variables, _ = self.lstm_layer(lstm_input)\n",
    "        model_outputs = self.linear_layer(hidden_variables[0])\n",
    "        return model_outputs\n",
    "    \n",
    "    def sample(self, input_features, lstm_states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_indices = []\n",
    "        lstm_inputs = input_features.unsqueeze(1)\n",
    "        for i in range(self.max_seq_len):\n",
    "            hidden_variables, lstm_states = self.lstm_layer(lstm_inputs, lstm_states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            model_outputs = self.linear_layer(hidden_variables.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted_outputs = model_outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_indices.append(predicted_outputs)\n",
    "            lstm_inputs = self.embedding_layer(predicted_outputs)                       # inputs: (batch_size, embed_size)\n",
    "            lstm_inputs = lstm_inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_indices = torch.stack(sampled_indices, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c6fb2-2fcb-4e50-aeaf-cc5e7536fa94",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b8de085-26f5-421a-b8e7-ca9df7068a3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_dir/vocabulary.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m      8\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([ \n\u001b[0;32m      9\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[0;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(), \n\u001b[0;32m     11\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(), \n\u001b[0;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m), \n\u001b[0;32m     13\u001b[0m                          (\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m))])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Load vocabulary wrapper\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_dir/vocabulary.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     18\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Build data loader\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jihwan\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_dir/vocabulary.pkl'"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not os.path.exists('models_dir/'):\n",
    "    os.makedirs('models_dir/')\n",
    "    \n",
    "# Image preprocessing, normalization for the pretrained resnet\n",
    "transform = transforms.Compose([ \n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "\n",
    "# Load vocabulary wrapper\n",
    "with open('data_dir/vocabulary.pkl', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "# Build data loader\n",
    "custom_data_loader = get_loader('data_dir/resized_images', 'data_dir/annotations/captions_train2014.json', vocabulary, \n",
    "                         transform, 128,\n",
    "                         shuffle=True, num_workers=0) \n",
    "\n",
    "# Build the models\n",
    "encoder_model = CNNModel(256).to(device)\n",
    "decoder_model = LSTMModel(256, 512, len(vocabulary), 1).to(device)\n",
    " \n",
    "    \n",
    "# Loss and optimizer\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "parameters = list(decoder_model.parameters()) + list(encoder_model.linear_layer.parameters()) + list(encoder_model.batch_norm.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.001)\n",
    "\n",
    "\n",
    "# Train the models\n",
    "total_num_steps = len(custom_data_loader)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for i, (imgs, caps, lens) in enumerate(custom_data_loader):\n",
    " \n",
    "        # Set mini-batch dataset\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        tgts = pack_padded_sequence(caps, lens, batch_first=True)[0]\n",
    " \n",
    "        # Forward, backward and optimize\n",
    "        feats = encoder_model(imgs)\n",
    "        outputs = decoder_model(feats, caps, lens)\n",
    "        loss = loss_criterion(outputs, tgts)\n",
    "        decoder_model.zero_grad()\n",
    "        encoder_model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        # Print log info\n",
    "        if i % 300 == 0 or i == total_num_steps - 1:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                  .format(epoch, 5, i, total_num_steps, loss.item(),\n",
    "                          np.exp(loss.item()))) \n",
    " \n",
    "        # Save the model checkpoints\n",
    "        if (i+1) % 1000 == 0:\n",
    "            torch.save(decoder_model.state_dict(), os.path.join(\n",
    "                'models_dir/', 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "            torch.save(encoder_model.state_dict(), os.path.join(\n",
    "                'models_dir/', 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a3be5-8334-4f95-a3e3-c2fe21856796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
