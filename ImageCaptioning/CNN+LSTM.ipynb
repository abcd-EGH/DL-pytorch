{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "10f349b8-ccc5-4625-82c2-ecc8cb1538d6",
      "metadata": {
        "id": "10f349b8-ccc5-4625-82c2-ecc8cb1538d6"
      },
      "source": [
        "# Download Dataset & Extract Zip File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e17c527e-c863-478c-82d6-fef3167bc675",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e17c527e-c863-478c-82d6-fef3167bc675",
        "outputId": "059de513-e483-4d49-de7f-4e2b4d4b9760",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "http://images.cocodataset.org/annotations/annotations_trainval2014.zip 다운로드 중 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "annotations_trainval2014.zip: 253MB [00:05, 49.8MB/s]                           \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "압축을 푸는 중 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6/6 [00:05<00:00,  1.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "http://images.cocodataset.org/zips/train2014.zip 다운로드 중 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train2014.zip: 13.5GB [04:58, 45.2MB/s]                            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "압축을 푸는 중 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 82784/82784 [01:42<00:00, 811.01it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "http://images.cocodataset.org/zips/val2014.zip 다운로드 중 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val2014.zip: 6.65GB [02:21, 46.9MB/s]                            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "압축을 푸는 중 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40505/40505 [00:55<00:00, 727.31it/s]\n"
          ]
        }
      ],
      "source": [
        "# COCO Dataset 다운로드\n",
        "\n",
        "import urllib\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "#https://stackoverflow.com/a/53877507/1558946\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "def download_data(url):\n",
        "    print(f\"{url} 다운로드 중 ...\")\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        zip_path, _ = urllib.request.urlretrieve(url, reporthook=t.update_to)\n",
        "\n",
        "    print(\"압축을 푸는 중 ...\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as f:\n",
        "        for name in tqdm(iterable=f.namelist(), total=len(f.namelist())):\n",
        "            f.extract(member=name, path=\"data_dir\")\n",
        "\n",
        "\n",
        "download_data(\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\")\n",
        "download_data(\"http://images.cocodataset.org/zips/train2014.zip\")\n",
        "download_data(\"http://images.cocodataset.org/zips/val2014.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0908e517-9cfa-477e-b212-ad8ed2044a29",
      "metadata": {
        "id": "0908e517-9cfa-477e-b212-ad8ed2044a29"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62a66548-4e4f-47d0-ba33-6bfa08b068e2",
      "metadata": {
        "id": "62a66548-4e4f-47d0-ba33-6bfa08b068e2"
      },
      "source": [
        "## Text (Caption) Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "41a4cf18-53bd-4fb0-8f09-4b1ee31efeff",
      "metadata": {
        "id": "41a4cf18-53bd-4fb0-8f09-4b1ee31efeff"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from pycocotools.coco import COCO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn.utils.rnn import pack_padded_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "321a8622-bb23-4086-9528-65eb44e079f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "321a8622-bb23-4086-9528-65eb44e079f2",
        "outputId": "7a1615b3-f9d9-4a0a-c8ed-6b0e21a3a4c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# punkt tokenizer 모델을 통해 주어진 텍스트를 구성된 단어로 토큰화\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d90ac7be-ff61-404e-99ce-f737e38740f9",
      "metadata": {
        "id": "d90ac7be-ff61-404e-99ce-f737e38740f9"
      },
      "outputs": [],
      "source": [
        "class Vocab(object):\n",
        "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.w2i = {}\n",
        "        self.i2w = {}\n",
        "        self.index = 0\n",
        "\n",
        "    def __call__(self, token):\n",
        "        if not token in self.w2i:\n",
        "            return self.w2i['<unk>']\n",
        "        return self.w2i[token]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.w2i)\n",
        "    def add_token(self, token):\n",
        "        if not token in self.w2i:\n",
        "            self.w2i[token] = self.index\n",
        "            self.i2w[self.index] = token\n",
        "            self.index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "77c11533-ee99-4dde-837a-613537cbbe7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77c11533-ee99-4dde-837a-613537cbbe7e",
        "outputId": "038b1bda-c4bf-4b7e-8660-8e9a258f1cf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.17s)\n",
            "creating index...\n",
            "index created!\n",
            "[100000/414113] Tokenized the captions.\n",
            "[200000/414113] Tokenized the captions.\n",
            "[300000/414113] Tokenized the captions.\n",
            "[400000/414113] Tokenized the captions.\n",
            "Total vocabulary size: 9948\n",
            "Saved the vocabulary wrapper to './data_dir/vocabulary.pkl'\n"
          ]
        }
      ],
      "source": [
        "# 실제 텍스트 토큰(단어 등)을 숫자 토큰으로 전환할 수 있는 사전 구축\n",
        "def build_vocabulary(json, threshold):\n",
        "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
        "    # Load JSON Text Annotation\n",
        "    coco = COCO(json)\n",
        "    counter = Counter()\n",
        "    ids = coco.anns.keys()\n",
        "    for i, id in enumerate(ids):\n",
        "        # Tokenize each word in Annotation/Caption (Convert to numbers) and Save to Counter\n",
        "        caption = str(coco.anns[id]['caption'])\n",
        "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "        counter.update(tokens)\n",
        "\n",
        "        if (i+1) % 100000 == 0:\n",
        "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
        "\n",
        "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
        "    tokens = [token for token, cnt in counter.items() if cnt >= threshold]\n",
        "\n",
        "    # Create a vocab wrapper and add some special tokens.\n",
        "    vocab = Vocab()\n",
        "    vocab.add_token('<pad>')\n",
        "    vocab.add_token('<start>')\n",
        "    vocab.add_token('<end>')\n",
        "    vocab.add_token('<unk>')\n",
        "\n",
        "    # Add the words to the vocabulary.\n",
        "    for i, token in enumerate(tokens):\n",
        "        vocab.add_token(token)\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocabulary(json='data_dir/annotations/captions_train2014.json', threshold=4)\n",
        "vocab_path = './data_dir/vocabulary.pkl'\n",
        "with open(vocab_path, 'wb') as f:\n",
        "    pickle.dump(vocab, f)\n",
        "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
        "print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f51b3f-84d2-49ad-8f4d-ab831c5d26cf",
      "metadata": {
        "id": "42f51b3f-84d2-49ad-8f4d-ab831c5d26cf"
      },
      "source": [
        "## Image Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fe2715e8-ae63-4ee9-a644-be83ea16c769",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe2715e8-ae63-4ee9-a644-be83ea16c769",
        "outputId": "11f62ba4-ce13-4657-d03b-5788e41f8f15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10000/82783] Resized the images and saved into './data_dir/resized_images/'.\n",
            "[20000/82783] Resized the images and saved into './data_dir/resized_images/'.\n",
            "[30000/82783] Resized the images and saved into './data_dir/resized_images/'.\n",
            "[40000/82783] Resized the images and saved into './data_dir/resized_images/'.\n",
            "[50000/82783] Resized the images and saved into './data_dir/resized_images/'.\n",
            "[60000/82783] Resized the images and saved into './data_dir/resized_images/'.\n",
            "[70000/82783] Resized the images and saved into './data_dir/resized_images/'.\n",
            "[80000/82783] Resized the images and saved into './data_dir/resized_images/'.\n"
          ]
        }
      ],
      "source": [
        "# 고정된 image_shape로 reshape\n",
        "def reshape_image(image, shape):\n",
        "    \"\"\"Resize an image to the given shape.\"\"\"\n",
        "    # return image.resize(shape, Image.ANTIALIAS) # Pillow 10.0.0에서 'ANTIALIAS'의 모듈이 제거되었음\n",
        "    return image.resize(shape, Image.LANCZOS)\n",
        "\n",
        "def reshape_images(image_path, output_path, shape):\n",
        "    \"\"\"Reshape the images in 'image_path' and save into 'output_path'.\"\"\"\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "\n",
        "    images = os.listdir(image_path)\n",
        "    num_im = len(images)\n",
        "    for i, im in enumerate(images):\n",
        "        with open(os.path.join(image_path, im), 'r+b') as f:\n",
        "            with Image.open(f) as image:\n",
        "                image = reshape_image(image, shape)\n",
        "                image.save(os.path.join(output_path, im), image.format)\n",
        "        if (i+1) % 10000 == 0:\n",
        "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
        "                   .format(i+1, num_im, output_path))\n",
        "\n",
        "image_path = './data_dir/train2014/'\n",
        "output_path = './data_dir/resized_images/'\n",
        "image_shape = [256, 256]\n",
        "reshape_images(image_path, output_path, image_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8951e90d-b603-4fc0-8c69-329f000cde2e",
      "metadata": {
        "id": "8951e90d-b603-4fc0-8c69-329f000cde2e"
      },
      "source": [
        "# Define Image Caption Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b2f55916-f837-40ca-8b1a-ba7e6f60dbe8",
      "metadata": {
        "id": "b2f55916-f837-40ca-8b1a-ba7e6f60dbe8"
      },
      "outputs": [],
      "source": [
        "class CustomCocoDataset(data.Dataset):\n",
        "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
        "    def __init__(self, data_path, coco_json_path, vocabulary, transform=None):\n",
        "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
        "\n",
        "        Args:\n",
        "            root: image directory.\n",
        "            json: coco annotation file path.\n",
        "            vocab: vocabulary wrapper.\n",
        "            transform: image transformer.\n",
        "        \"\"\"\n",
        "        self.root = data_path\n",
        "        self.coco_data = COCO(coco_json_path)\n",
        "        self.indices = list(self.coco_data.anns.keys())\n",
        "        self.vocabulary = vocabulary\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
        "        coco_data = self.coco_data\n",
        "        vocabulary = self.vocabulary\n",
        "        annotation_id = self.indices[idx]\n",
        "        caption = coco_data.anns[annotation_id]['caption']\n",
        "        image_id = coco_data.anns[annotation_id]['image_id']\n",
        "        image_path = coco_data.loadImgs(image_id)[0]['file_name']\n",
        "\n",
        "        image = Image.open(os.path.join(self.root, image_path)).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert caption (string) to word ids.\n",
        "        word_tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
        "        caption = []\n",
        "        caption.append(vocabulary('<start>'))\n",
        "        caption.extend([vocabulary(token) for token in word_tokens])\n",
        "        caption.append(vocabulary('<end>'))\n",
        "        ground_truth = torch.Tensor(caption)\n",
        "        return image, ground_truth\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "977355a7-524f-40d3-b618-399eacb797ec",
      "metadata": {
        "id": "977355a7-524f-40d3-b618-399eacb797ec"
      },
      "outputs": [],
      "source": [
        "def collate_function(data_batch):\n",
        "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
        "\n",
        "    We should build custom collate_fn rather than using default collate_fn,\n",
        "    because merging caption (including padding) is not supported in default.\n",
        "    Args:\n",
        "        data: list of tuple (image, caption).\n",
        "            - image: torch tensor of shape (3, 256, 256).\n",
        "            - caption: torch tensor of shape (?); variable length.\n",
        "    Returns:\n",
        "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
        "        targets: torch tensor of shape (batch_size, padded_length).\n",
        "        lengths: list; valid length for each padded caption.\n",
        "    \"\"\"\n",
        "    # Sort a data list by caption length (descending order).\n",
        "    data_batch.sort(key=lambda d: len(d[1]), reverse=True)\n",
        "    imgs, caps = zip(*data_batch)\n",
        "\n",
        "    # Merge images (from list of 3D tensors to 4D tensor).\n",
        "    # Originally, imgs is a list of <batch_size> number of RGB images with dimensions (3, 256, 256)\n",
        "    # This line of code turns it into a single tensor of dimensions (<batch_size>, 3, 256, 256)\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "\n",
        "    # Merge captions (from list of 1D tensors to 2D tensor), similar to merging of images donw above.\n",
        "    cap_lens = [len(cap) for cap in caps]\n",
        "    tgts = torch.zeros(len(caps), max(cap_lens)).long()\n",
        "    for i, cap in enumerate(caps):\n",
        "        end = cap_lens[i]\n",
        "        tgts[i, :end] = cap[:end]\n",
        "    return imgs, tgts, cap_lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6c0bebc3-42ad-40b7-8fbe-9a490be29ba1",
      "metadata": {
        "id": "6c0bebc3-42ad-40b7-8fbe-9a490be29ba1"
      },
      "outputs": [],
      "source": [
        "# COCO Dataset을 위한 맞춤형 데이터 로더를 반환하는 get_loader 함수 구현\n",
        "def get_loader(data_path, coco_json_path, vocabulary, transform, batch_size, shuffle, num_workers):\n",
        "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
        "    # COCO caption dataset\n",
        "    coco_dataset = CustomCocoDataset(data_path=data_path,\n",
        "                       coco_json_path=coco_json_path,\n",
        "                       vocabulary=vocabulary,\n",
        "                       transform=transform)\n",
        "\n",
        "    # Data loader for COCO dataset\n",
        "    # This will return (images, captions, lengths) for each iteration.\n",
        "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
        "    # captions: a tensor of shape (batch_size, padded_length).\n",
        "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
        "    custom_data_loader = torch.utils.data.DataLoader(dataset=coco_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=shuffle,\n",
        "                                              num_workers=num_workers,\n",
        "                                              collate_fn=collate_function)\n",
        "    return custom_data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5931e7b-d0f2-45b6-aed1-8240a1a1d5ed",
      "metadata": {
        "id": "e5931e7b-d0f2-45b6-aed1-8240a1a1d5ed"
      },
      "source": [
        "# Define Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ab0d483-1b9c-40b0-94ac-da6cf2a7d5fe",
      "metadata": {
        "id": "9ab0d483-1b9c-40b0-94ac-da6cf2a7d5fe"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "80cc2415-1e41-4ae5-b933-e96ba17d0091",
      "metadata": {
        "id": "80cc2415-1e41-4ae5-b933-e96ba17d0091"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, embedding_size):\n",
        "        \"\"\"Load the pretrained efficientnet and replace top fc layer.\"\"\"\n",
        "        super(CNNModel, self).__init__()\n",
        "        efficientnet = models.efficientnet_b4(weights='IMAGENET1K_V1')\n",
        "        module_list = list(efficientnet.children())[:-1]      # delete the last dropout & fc layer.\n",
        "        last_layer = list(efficientnet.children())[-1]      # fc layer of efficientnet = list(efficientnet.children())[-1]\n",
        "        self.efficientnet_module = nn.Sequential(*module_list, last_layer[0]) # add the pretrained last dropout layer.\n",
        "        self.linear_layer = nn.Linear(last_layer[1].in_features, embedding_size)\n",
        "        self.batch_norm = nn.BatchNorm1d(embedding_size, momentum=0.01)\n",
        "\n",
        "    def forward(self, input_images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            efficientnet_features = self.efficientnet_module(input_images)\n",
        "        efficientnet_features = efficientnet_features.reshape(efficientnet_features.size(0), -1)\n",
        "        final_features = self.batch_norm(self.linear_layer(efficientnet_features))\n",
        "        return final_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49e717bf-b7d1-4705-828b-24e1aca53573",
      "metadata": {
        "id": "49e717bf-b7d1-4705-828b-24e1aca53573"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2393be34-ec81-43f0-bd04-c8339d6fef1a",
      "metadata": {
        "id": "2393be34-ec81-43f0-bd04-c8339d6fef1a"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_layer_size, vocabulary_size, num_layers, max_seq_len=20):\n",
        "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_size)\n",
        "        self.lstm_layer = nn.LSTM(embedding_size, hidden_layer_size, num_layers, batch_first=True)\n",
        "        self.linear_layer = nn.Linear(hidden_layer_size, vocabulary_size)\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def forward(self, input_features, capts, lens):\n",
        "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
        "        embeddings = self.embedding_layer(capts)\n",
        "        embeddings = torch.cat((input_features.unsqueeze(1), embeddings), 1)\n",
        "        lstm_input = pack_padded_sequence(embeddings, lens, batch_first=True)\n",
        "        hidden_variables, _ = self.lstm_layer(lstm_input)\n",
        "        model_outputs = self.linear_layer(hidden_variables[0])\n",
        "        return model_outputs\n",
        "\n",
        "    def sample(self, input_features, lstm_states=None):\n",
        "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
        "        sampled_indices = []\n",
        "        lstm_inputs = input_features.unsqueeze(1)\n",
        "        for i in range(self.max_seq_len):\n",
        "            hidden_variables, lstm_states = self.lstm_layer(lstm_inputs, lstm_states)          # hiddens: (batch_size, 1, hidden_size)\n",
        "            model_outputs = self.linear_layer(hidden_variables.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
        "            _, predicted_outputs = model_outputs.max(1)                        # predicted: (batch_size)\n",
        "            sampled_indices.append(predicted_outputs)\n",
        "            lstm_inputs = self.embedding_layer(predicted_outputs)                       # inputs: (batch_size, embed_size)\n",
        "            lstm_inputs = lstm_inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
        "        sampled_indices = torch.stack(sampled_indices, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
        "        return sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a8c6fb2-2fcb-4e50-aeaf-cc5e7536fa94",
      "metadata": {
        "id": "6a8c6fb2-2fcb-4e50-aeaf-cc5e7536fa94"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3b8de085-26f5-421a-b8e7-ca9df7068a3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "3b8de085-26f5-421a-b8e7-ca9df7068a3c",
        "outputId": "009ed995-ffd9-49c4-e01d-145112e3e4f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.97s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/3236 [00:01<1:01:40,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0/5], Step [0/3236], Loss: 9.2070, Perplexity: 9966.2886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 113/3236 [01:01<28:28,  1.83it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-79d50ea41ab8>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Set mini-batch dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-99677e59bc02>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoco_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadImgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3480\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3482\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# set batch size\n",
        "batch_size = 1024\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if not os.path.exists('models_dir/'):\n",
        "    os.makedirs('models_dir/')\n",
        "\n",
        "# Image preprocessing, normalization for the pretrained resnet\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "# Load vocabulary wrapper\n",
        "with open('data_dir/vocabulary.pkl', 'rb') as f:\n",
        "    vocabulary = pickle.load(f)\n",
        "\n",
        "# Build data loader\n",
        "custom_data_loader = get_loader('data_dir/resized_images', 'data_dir/annotations/captions_train2014.json', vocabulary,\n",
        "                         transform, batch_size,\n",
        "                         shuffle=True, num_workers=0)\n",
        "\n",
        "# Build the models\n",
        "encoder_model = CNNModel(256).to(device)\n",
        "decoder_model = LSTMModel(256, 512, len(vocabulary), 1).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "parameters = list(decoder_model.parameters()) + list(encoder_model.linear_layer.parameters()) + list(encoder_model.batch_norm.parameters())\n",
        "optimizer = torch.optim.Adam(parameters, lr=0.001)\n",
        "\n",
        "\n",
        "# Train the models\n",
        "total_num_steps = len(custom_data_loader)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for i, (imgs, caps, lens) in enumerate(tqdm(custom_data_loader)):\n",
        "\n",
        "        # Set mini-batch dataset\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        tgts = pack_padded_sequence(caps, lens, batch_first=True)[0]\n",
        "\n",
        "        # Forward, backward and optimize\n",
        "        feats = encoder_model(imgs)\n",
        "        outputs = decoder_model(feats, caps, lens)\n",
        "        loss = loss_criterion(outputs, tgts)\n",
        "        decoder_model.zero_grad()\n",
        "        encoder_model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print log info\n",
        "        if i % 300 == 0 or i == total_num_steps - 1:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
        "                  .format(epoch, 5, i, total_num_steps, loss.item(),\n",
        "                          np.exp(loss.item())))\n",
        "\n",
        "        # Save the model checkpoints\n",
        "        if (i+1) % 1000 == 0:\n",
        "            torch.save(decoder_model.state_dict(), os.path.join(\n",
        "                'models_dir/', 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "            torch.save(encoder_model.state_dict(), os.path.join(\n",
        "                'models_dir/', 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc6a3be5-8334-4f95-a3e3-c2fe21856796",
      "metadata": {
        "id": "fc6a3be5-8334-4f95-a3e3-c2fe21856796"
      },
      "outputs": [],
      "source": [
        "image_file_path = 'sample.jpg'\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def load_image(image_file_path, transform=None):\n",
        "    img = Image.open(image_file_path).convert('RGB')\n",
        "    img = img.resize([224, 224], Image.LANCZOS)\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img).unsqueeze(0)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# Image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "# Load vocabulary wrapper\n",
        "with open('data_dir/vocabulary.pkl', 'rb') as f:\n",
        "    vocabulary = pickle.load(f)\n",
        "\n",
        "\n",
        "# Build models\n",
        "encoder_model = CNNModel(256).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
        "decoder_model = LSTMModel(256, 512, len(vocabulary), 1)\n",
        "encoder_model = encoder_model.to(device)\n",
        "decoder_model = decoder_model.to(device)\n",
        "\n",
        "\n",
        "# Load the trained model parameters\n",
        "encoder_model.load_state_dict(torch.load('models_dir/encoder-2-3000.ckpt'))\n",
        "decoder_model.load_state_dict(torch.load('models_dir/decoder-2-3000.ckpt'))\n",
        "\n",
        "# Prepare an image\n",
        "img = load_image(image_file_path, transform)\n",
        "img_tensor = img.to(device)\n",
        "\n",
        "\n",
        "# Generate an caption from the image\n",
        "feat = encoder_model(img_tensor)\n",
        "sampled_indices = decoder_model.sample(feat)\n",
        "sampled_indices = sampled_indices[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
        "\n",
        "\n",
        "# Convert word_ids to words\n",
        "predicted_caption = []\n",
        "for token_index in sampled_indices:\n",
        "    word = vocabulary.i2w[token_index]\n",
        "    predicted_caption.append(word)\n",
        "    if word == '<end>':\n",
        "        break\n",
        "predicted_sentence = ' '.join(predicted_caption)\n",
        "\n",
        "\n",
        "# Print out the image and the generated caption\n",
        "%matplotlib inline\n",
        "print (predicted_sentence)\n",
        "img = Image.open(image_file_path)\n",
        "plt.imshow(np.asarray(img))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
